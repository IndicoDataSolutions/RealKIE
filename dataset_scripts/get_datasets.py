import json
import os
import hashlib
import traceback
import time

import fire
import pandas as pd
from indico import IndicoClient, IndicoConfig
from indico.client import GraphQLRequest, RequestChain, Debouncer
from indico.queries import GetExport, DownloadExport, GetDataset, RetrieveStorageObject
from sklearn.model_selection import train_test_split
import gzip
import tqdm


class ClientWithSlowRetry(IndicoClient):
    def call(self, *args, **kwargs):
        try:
            return super().call(*args, **kwargs)
        except TypeError:
            print("Retrying the following exception")
            print(traceback.format_exc())
            time.sleep(60)
            return self.call(*args, **kwargs)


class GraphQLMagic(GraphQLRequest):
    def __init__(self, *args, **kwargs):
        super().__init__(query=self.query, variables=kwargs)


class _CreateExportSimple(GraphQLMagic):
    query = """
    mutation createExport($datasetId: Int!, $labelsetId: Int!) {
        createExport(anonymous: false, datasetId: $datasetId, labelsetId: $labelsetId, labeledOnly: true, name: "Export generated by get_datasets script") {
            id
        }
    }
    """

class _CreateExportWithPreds(GraphQLMagic):
    query = """
    mutation createExport($datasetId: Int!, $labelsetId: Int!, $modelId: Int!) {
        createExport(
            anonymous: false,
            datasetId: $datasetId,
            labelsetId: $labelsetId,
            labeledOnly: false,
            modelIds: [$modelId],
            name: "Export generated by get_datasets script"
        ) {
            id
        }
    }
    """

class CreateExportSimple(RequestChain):
    previous = None

    def __init__(self, dataset_id: int, labelset_id: int, model_id: list=None, wait: bool = True):
        self.dataset_id = dataset_id
        self.labelset_id = labelset_id
        self.wait = wait
        self.model_id = model_id
        super().__init__()

    def requests(self):
        if self.model_id is not None:
            yield _CreateExportWithPreds(
                datasetId=self.dataset_id,
                labelsetId=self.labelset_id,
                modelId=self.model_id,
            )
        else:
            yield _CreateExportSimple(
                datasetId=self.dataset_id, labelsetId=self.labelset_id
            )
        yield GetExport(self.previous["createExport"]["id"])
        debouncer = Debouncer()
        if self.wait is True:
            while self.previous.status not in ["COMPLETE", "FAILED"]:
                yield GetExport(self.previous.id)
                debouncer.backoff()
        yield GetExport(self.previous.id)


class GetDatafileByID(GraphQLMagic):
    query = """
    query getDatafileById($datafileId: Int!) {
        datafile(datafileId: $datafileId) {
            pages {
            id
            pageInfo
            image
            }
        }
    }
    """


def get_export(client, dataset_id, labelset_id, model_id=None):
    # Get dataset object
    dataset = client.call(GetDataset(id=dataset_id))

    # Create export object using dataset's id and labelset id
    export = client.call(
        CreateExportSimple(dataset_id=dataset.id, labelset_id=labelset_id, model_id=model_id)
    )
    # Use export object to download as pandas csv
    csv = client.call(DownloadExport(export.id))
    csv = csv.rename(columns=lambda col: col.rsplit("_", 1)[0])
    return csv


def reformat_labels(labels, document):
    spans_labels = json.loads(labels)
    old_labels_i = []
    for target in spans_labels["targets"]:
        old_labels_i.append(
            {
                "label": target["label"],
                "start": min(l["start"] for l in target["spans"]),
                "end": max(l["end"] for l in target["spans"]),
            }
        )
        old_labels_i[-1]["text"] = document[
            old_labels_i[-1]["start"] : old_labels_i[-1]["end"]
        ]
    return json.dumps(old_labels_i)

def get_ocr_by_datafile_id(client, datafile_id):
    """
    Given an Indico client and a datafile ID, download OCR data for all pages
    along with page image PNGs for each page.
    """
    datafile_meta = client.call(GetDatafileByID(datafileId=datafile_id))
    page_ocrs, page_images = [], []
    for page in datafile_meta["datafile"]["pages"]:
        page_info = client.call(RetrieveStorageObject(page["pageInfo"]))
        # Could just return page image and save to file in inner loop if required
        page_image = client.call(RetrieveStorageObject(page["image"]))
        page_ocrs.append(page_info)
        page_images.append(page_image)
    return page_ocrs, page_images


def get_same_splits(dataset_dir, split_file, records):
    filenames = set(
        pd.read_csv(
            os.path.join(dataset_dir, "old_split_files", split_file)
        ).original_filename
    )
    return [r for r in records if r["original_filename"] in filenames]


def cleanup_path(path):
    if path.startswith("datasets/"):
        path = path[len("datasets/") :]
    return path


def get_dataset(
    name,
    dataset_id,
    labelset_id,
    model_id=None,
    pred_col="predictions",    
    label_col="labels",
    text_col="text",
    host="app.indico.io",
    api_token_path="/home/m/api_keys/prod_api_token.txt",
    update=False,
):
    dataset_dir = os.path.join("datasets", name)
    if not os.path.exists(dataset_dir):
        os.mkdir(dataset_dir)
        os.mkdir(os.path.join(dataset_dir, "images"))
        os.mkdir(os.path.join(dataset_dir, "files"))
        os.mkdir(os.path.join(dataset_dir, "ocr"))

    if os.path.exists(os.path.join(dataset_dir, "val.csv")) or os.path.exists(
        os.path.join(dataset_dir, "old_split_files")
    ):
        if not update:
            print("This dataset has already been downloaded.")
            exit()

        else:
            if not os.path.exists(os.path.join(dataset_dir, "old_split_files")):
                os.mkdir(os.path.join(dataset_dir, "old_split_files"))
            for f in ["val.csv", "train.csv", "test.csv", "raw_export.csv"]:
                if os.path.exists(os.path.join(dataset_dir, f)):
                    os.rename(
                        os.path.join(dataset_dir, f),
                        os.path.join(dataset_dir, "old_split_files", f),
                    )
    else:
        if update:
            print("You cannot update a dataset that does not exist")
            exit()

    my_config = IndicoConfig(host=host, api_token_path=api_token_path)
    client = ClientWithSlowRetry(config=my_config)

    export_path = os.path.join(dataset_dir, "raw_export.csv")

    if not os.path.exists(export_path):
        print("Creating and Downloading Export")
        raw_export = get_export(client, dataset_id, labelset_id)
        raw_export.to_csv(export_path)
    else:
        raw_export = pd.read_csv(export_path)
    if label_col not in raw_export.columns or text_col not in raw_export.columns:
        print(
            f"--label_col {label_col} or --text_col not found in export columns {raw_export.columns}"
        )
        exit(1)

    records = raw_export.to_dict("records")
    output_records = []

    for i, row in tqdm.tqdm(enumerate(records)):
        print(row.keys())
        if pd.isna(row[label_col]):
            print("No labels - skipping")
            continue
        filename = str(
            hashlib.md5(
                f"{row['file_id']} {row['file_name']}".encode("utf-8")
            ).hexdigest()
        )
        file_dir = os.path.join(dataset_dir, "images", filename)
        os.makedirs(file_dir, exist_ok=True)
        local_page_pattern = os.path.join(file_dir, "page_{}.png")
        datafile_meta = client.call(GetDatafileByID(datafileId=row["file_id"]))
        image_files = []
        page_ocrs = []

        ocr_path = os.path.join(dataset_dir, "ocr", filename + "." + "json.gz")
        if not os.path.exists(ocr_path):
            for page in datafile_meta["datafile"]["pages"]:
                if not os.path.exists(ocr_path):
                    page_ocr = client.call(RetrieveStorageObject(page["pageInfo"]))
                    page_ocrs.append(page_ocr)
            with gzip.open(ocr_path, "wt") as fp:
                fp.write(json.dumps(page_ocrs))

        for i, page in enumerate(datafile_meta["datafile"]["pages"]):
            local_page_image = local_page_pattern.format(i)
            image_files.append(cleanup_path(local_page_image))
            if not os.path.exists(local_page_image):
                page_image = client.call(RetrieveStorageObject(page["image"]))
                with open(local_page_image, "wb") as fp:
                    fp.write(page_image)

        document_path = os.path.join(
            dataset_dir, "files", filename + "." + row["file_name"].split(".")[-1]
        )
        output_record = {
            "original_filename": row["file_name"],
            "ocr": cleanup_path(ocr_path),
            "text": row[text_col],
            "labels": reformat_labels(row[label_col], row[text_col]),
            "image_files": json.dumps(image_files),
            "document_path": cleanup_path(document_path),
            **({}if model_id is None else {"predictions": reformat_labels(row[pred_col], row[text_col])})
        }
        if not os.path.exists(document_path):
            with open(document_path, "wb") as fp:
                try:
                    fp.write(client.call(RetrieveStorageObject(row["file_url"])))
                except:
                    print("Failed downloading original file")
                    output_record["document_path"] = None
        output_records.append(output_record)
    if update:
        val_records = get_same_splits(dataset_dir, "val.csv", output_records)
        train_records = get_same_splits(dataset_dir, "train.csv", output_records)
        test_records = get_same_splits(dataset_dir, "test.csv", output_records)
    else:
        train_records, test_val_records = train_test_split(
            output_records, test_size=0.4
        )
        test_records, val_records = train_test_split(test_val_records, test_size=0.5)
    for split, records in [
        ("train", train_records),
        ("test", test_records),
        ("val", val_records),
    ]:
        pd.DataFrame.from_records(records).to_csv(
            os.path.join(dataset_dir, "{}.csv".format(split))
        )
    pd.DataFrame.from_records(output_records).to_csv(os.path.join(dataset_dir, "all.csv"))


if __name__ == "__main__":
    fire.Fire(get_dataset)
